---
title: "Cooling Center Optimization Generateing OSRM Matrices"
output: html_notebook
---

This R notebook will contain the code chunks for preparing spatial data and generating the OSRM distance matrices, which will be used in the location modeling later on.  

---
title: OSRM Distance Matrix Generation for Phoenix Cooling Centers
author: Lance Watkins
date: July 22, 2025
output: html_notebook
---

```{r setup, include=FALSE}
# Setup chunk: This runs R code silently when knitting/running notebook
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```


## 1. Project Setup and Data Paths

This section defines the working directory and paths for input/output data. Ensure your Docker OSRM servers are running before executing the OSRM query chunks!

```{r message=TRUE}
# IMPORTANT: Change this path to where your R project is located.
# This directory should contain your .Rmd file and a 'data' subfolder for inputs/outputs.
setwd("C:/Users/raelu/ASU Dropbox/Lance Watkins/ASU/projects/CoolingCenterSpatiaOptimization/CoolingCenterOpt-R-SPOPT")

# Create a 'data' sub-directory if it doesn't exist
if (!dir.exists("data")) {
  dir.create("data")
}

# Define paths for your input CSV files (assuming they are in the 'data' subfolder)
input_data_path <- "data/"
output_data_path <- "data/" # Where to save the generated .rds files

message("Project setup complete. Data paths defined.")
```

## 2. Install and Load Required R Packages

Ensure these packages are installed. If not, uncomment and run install.packages() first.

```{r include=FALSE}
# install.packages(c("sf", "osrm", "dplyr", "lpSolve", "R.utils"))

library(sf)      # For Simple Features (spatial data handling)
library(osrm)    # To interface with the OSRM server
library(dplyr)   # For data manipulation (e.g., mutate, bind_rows)
library(R.utils) # For the chunking function (used within safe_osrmTable)
library(lpSolve) # Will be needed for the MCLP optimization later on
```


## 3. Load and Prepare Spatial Data (Using CSV Files)

This section loads your spatial point data (candidate locations, existing centers, demand points) from CSV files and prepares them for OSRM.

```{r load_prepare_data_csvs, message=TRUE}
message("Loading and preparing spatial data from CSVs...")

# --- IMPORTANT: Ensure your CSV files are in the 'data/' subfolder ---
# And check that they have the required columns: 'id', 'latitude', 'longitude'.
# For demand_points.csv, also ensure a 'population' or 'weight' column.

# Load Candidate Locations
candidate_locs <- read.csv(paste0(input_data_path, "candidate_locations.csv")) %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>% # crs 4326 is WGS84 (lat/lon)
  # Ensure unique 'id' column is of character type. Adjust 'id' if your CSV column is named differently.
  dplyr::mutate(id = as.character(id))

# Load Existing Cooling Center Locations
existing_locs <- read.csv(paste0(input_data_path, "existing_cooling_centers.csv")) %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  # Ensure unique 'id' column is of character type. Adjust 'id' if your CSV column is named differently.
  dplyr::mutate(id = as.character(id)) %>%
  # Add an Address column so the columns align with candidate locations dataframe as well. For now populate with NA
  dplyr::mutate(address = NA) %>%
  dplyr::select(c("id","address","type","geometry"))

# Load Demand Points
demand_pts <- read.csv(paste0(input_data_path, "demand_points.csv")) %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  # Ensure unique 'id' column is of character type. Adjust 'id' if your CSV column is named differently.
  dplyr::mutate(id = as.character(id))



# Combine all potential facility locations (candidate + existing)
# This combined set will be the 'sources' for your OSRM queries.

all_facilities <- dplyr::bind_rows(candidate_locs, existing_locs) %>%
  # Add a simple numeric_id for consistent internal matrix indexing,
  # although we will primarily use the 'id' column for clarity in results.
  dplyr::mutate(numeric_id = dplyr::row_number())

# Optional: Basic check for unique IDs (good practice)
if (length(unique(all_facilities$id)) != nrow(all_facilities)) {
  warning("Duplicate 'id' values found in combined all_facilities. This might cause issues with matrix naming.")
}
if (length(unique(demand_pts$id)) != nrow(demand_pts)) {
  warning("Duplicate 'id' values found in demand_pts. This might cause issues with matrix naming.")
}

message("Spatial data loaded and prepared. Sample of all_facilities:")
print(head(all_facilities))
message("Sample of demand_pts:")
print(head(demand_pts))
```


## 4. Define Helper Function for OSRM Queries

This helper function wraps `osrmTable` calls, adding robustness through chunking queries and retries. This is crucial for large datasets. The code in this section defines a single function that acts as a robust wrapper around the standard osrmTable() function from the osrm package. Its main purpose is to make the process of querying the OSRM servers for large datasets more reliable and less prone to crashing or timing out.

This is achieved through two key mechanisms:

Chunking: It splits your large set of destination points (dst_sf) into smaller, more manageable chunks before sending them to the OSRM server.

Error Handling and Retries: It wraps each query in a tryCatch block, so if a query fails (e.g., due to a network hiccup or a server timeout), it automatically retries it a few times before giving up.

Function Arguments

First, let's look at the arguments the function accepts. These are the inputs you provide when you call it:

src_sf: The sf object for your source locations (your combined all_facilities).

dst_sf: The sf object for your destination locations (your demand_pts).

measure: What you want to calculate, e.g., c("distance", "duration").

server_url, server_port: The address and port of your local Docker OSRM server.

chunk_size: How many destination points to send in a single query.

retries: How many times to retry a failed query.

```{r osrm_helper_function, message=TRUE}

safe_osrmTable <- function(src_sf, dst_sf, measure = c("distance", "duration"),
                           server_url, server_port, chunk_size_src = 500, chunk_size_dst = 500, retries = 3) {

  options(osrm.server = server_url)

  num_sources <- nrow(src_sf)
  num_destinations <- nrow(dst_sf)

  if (num_sources == 0 || num_destinations == 0) {
    warning("Source or Destination SF object is empty. Returning empty matrices.")
    return(list(distances = matrix(NA, nrow = num_sources, ncol = num_destinations,
                                   dimnames = list(src_sf$id, dst_sf$id)),
                durations = matrix(NA, nrow = num_sources, ncol = num_destinations,
                                   dimnames = list(src_sf$id, dst_sf$id))))
  }

  all_distances <- matrix(NA, nrow = num_sources, ncol = num_destinations)
  all_durations <- matrix(NA, nrow = num_sources, ncol = num_destinations)

  rownames(all_distances) <- src_sf$id
  colnames(all_distances) <- dst_sf$id
  rownames(all_durations) <- src_sf$id
  colnames(all_durations) <- dst_sf$id
  
  src_chunks <- ceiling(num_sources / chunk_size_src)
  dst_chunks <- ceiling(num_destinations / chunk_size_dst)


  # This is the core of the function. The for loop iterates through each chunk of your demand points. Inside the loop, a while loop combined with tryCatch attempts to make a query for that chunk. If the query fails (due to a timeout, a bad connection, etc.), tryCatch "catches" the error and the while loop restarts the process, trying again up to the number of times you specified in the retries argument. 
  
    for (i in 1:src_chunks) {
    src_start <- (i - 1) * chunk_size_src + 1
    src_end <- min(i * chunk_size_src, num_sources)
    current_src_chunk <- src_sf[src_start:src_end, ]

    for (j in 1:dst_chunks) {
      dst_start <- (j - 1) * chunk_size_dst + 1
      dst_end <- min(j * chunk_size_dst, num_destinations)
      current_dst_chunk <- dst_sf[dst_start:dst_end, ]

      message(paste0("  Processing src chunk ", i, " of ", src_chunks,
                     " and dst chunk ", j, " of ", dst_chunks))

      attempt <- 1
      success <- FALSE
      while (attempt <= retries && !success) {
        tryCatch({
          od_chunk <- osrmTable(src = current_src_chunk, dst = current_dst_chunk, measure = measure)
          all_distances[src_start:src_end, dst_start:dst_end] <- od_chunk$distances
          all_durations[src_start:src_end, dst_start:dst_end] <- od_chunk$durations
          success <- TRUE
        }, error = function(e) {
          warning(paste0("    OSRM query failed (attempt ", attempt, "): ", e$message, " Retrying..."))
          attempt <<- attempt + 1
          Sys.sleep(5)
          if (attempt > retries) {
            stop(paste0("OSRM query failed after multiple retries for src chunk ", i, " and dst chunk ", j, ". Aborting."))
          }
        })
      }
    }
  }

  message("Successfully generated matrices.")
  return(list(distances = all_distances, durations = all_durations))
}
```

## 5. Calculate Network Distance Matrices using OSRM

This code chunk will use the safe_osrmTable() function to connect to your running OSRM servers and perform the actual distance and duration calculations. This is a computationally intensive process.

Important: Before you run this, please quickly check your PowerShell window and run docker ps to confirm that your osrm-car-server (on port 5000) and osrm-foot-server (on port 5001) are both Up.

**IMPORTANT:** Ensure your Docker OSRM servers (`osrm-car-server` on port 5000 and `osrm-foot-server` on port 5001) are running BEFORE executing these chunks!

```{r calculate_matrices, message=TRUE, warning=TRUE}

message("\n--- Starting OSRM Query for CAR Travel Matrix ---")
car_results <- safe_osrmTable(src = all_facilities, dst = demand_pts,
                              measure = c("distance", "duration"),
                              server_url = "http://localhost:5000/", # Car server port
                              server_port = 5000,
                              chunk_size_src = 500, # NEW: Chunk size for origin points
                              chunk_size_dst = 500) # NEW: Chunk size for destination points

distance_matrix_car <- car_results$distances # Distances in meters
duration_matrix_car <- car_results$durations # Durations in minutes
message("Car Travel Matrix generated.")


message("\n--- Starting OSRM Query for FOOT Travel Matrix ---")
foot_results <- safe_osrmTable(src = all_facilities, dst = demand_pts,
                               measure = c("distance", "duration"),
                               server_url = "http://localhost:5001/", # Foot server port
                               server_port = 5001,
                               chunk_size_src = 500, # NEW: Chunk size for origin points
                               chunk_size_dst = 500) # NEW: Chunk size for destination points

distance_matrix_foot <- foot_results$distances # Distances in meters
duration_matrix_foot <- foot_results$durations # Durations in minutes
message("Foot Travel Matrix generated.")
```

## Step 6: Save the Matrices and Spatial Data

This is the final, and very important, chunk of the data preparation process. It saves all your generated matrices and prepared sf objects to .rds files. The .rds file format is a native R binary format that allows for very fast loading, which is perfect for your Shiny app.

```{r save_data, message=TRUE}
message("\n--- Saving generated data to .rds files ---")

# Save the Car travel matrices
saveRDS(distance_matrix_car, paste0(output_data_path, "distance_matrix_car.rds"))
saveRDS(duration_matrix_car, paste0(output_data_path, "duration_matrix_car.rds"))

# Save the Foot travel matrices
saveRDS(distance_matrix_foot, paste0(output_data_path, "distance_matrix_foot.rds"))
saveRDS(duration_matrix_foot, paste0(output_data_path, "duration_matrix_foot.rds"))

# Also save the prepared sf objects, as Shiny will need these for plotting and details.
saveRDS(all_facilities, paste0(output_data_path, "all_facilities.rds"))
saveRDS(demand_pts, paste0(output_data_path, "demand_pts.rds"))


message("\n--- Saving generated data to CSV files ---")

# The `sf` objects need to be converted to data frames before saving to CSV.
# This removes the special geometry column that `write.csv()` cannot handle.

# Convert sf objects to data frames
all_facilities_df <- sf::st_drop_geometry(all_facilities)
demand_pts_df <- sf::st_drop_geometry(demand_pts)

# Save the data frames as CSV files
write.csv(all_facilities_df, paste0(output_data_path, "all_facilities.csv"), row.names = FALSE)
write.csv(demand_pts_df, paste0(output_data_path, "demand_pts.csv"), row.names = FALSE)
write.csv(distance_matrix_car, paste0(output_data_path, "distance_matrix_car.csv"))
write.csv(duration_matrix_car, paste0(output_data_path, "duration_matrix_car.csv"))
write.csv(distance_matrix_foot, paste0(output_data_path, "distance_matrix_foot.csv"))
write.csv(duration_matrix_foot, paste0(output_data_path, "duration_matrix_foot.csv"))


message("All data saved successfully in both .rds and .csv formats! You are now ready to build your Shiny dashboard.")
```





