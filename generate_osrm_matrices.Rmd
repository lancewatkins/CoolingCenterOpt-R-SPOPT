---
title: "Cooling Center Optimization Generateing OSRM Matrices"
output: html_notebook
---

This R notebook will contain the code chunks for preparing spatial data and generating the OSRM distance matrices, which will be used in the location modeling later on.  

---
title: OSRM Distance Matrix Generation for Phoenix Cooling Centers
author: Lance Watkins
date: July 22, 2025
output: html_notebook
---

```{r setup, include=FALSE}
# Setup chunk: This runs R code silently when knitting/running notebook
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```


## 1. Project Setup and Data Paths

This section defines the working directory and paths for input/output data. Ensure your Docker OSRM servers are running before executing the OSRM query chunks!

```{r message=TRUE}
# IMPORTANT: Change this path to where your R project is located.
# This directory should contain your .Rmd file and a 'data' subfolder for inputs/outputs.
setwd("C:/Users/raelu/ASU Dropbox/Lance Watkins/ASU/projects/CoolingCenterSpatiaOptimization/CoolingCenterOpt-R-SPOPT")

# Create a 'data' sub-directory if it doesn't exist
if (!dir.exists("data")) {
  dir.create("data")
}

# Define paths for your input CSV files (assuming they are in the 'data' subfolder)
input_data_path <- "data/"
output_data_path <- "data/" # Where to save the generated .rds files

message("Project setup complete. Data paths defined.")
```

## 2. Install and Load Required R Packages

Ensure these packages are installed. If not, uncomment and run install.packages() first.

```{r include=FALSE}
# install.packages(c("sf", "osrm", "dplyr", "lpSolve", "R.utils"))

library(sf)      # For Simple Features (spatial data handling)
library(osrm)    # To interface with the OSRM server
library(dplyr)   # For data manipulation (e.g., mutate, bind_rows)
library(R.utils) # For the chunking function (used within safe_osrmTable)
library(lpSolve) # Will be needed for the MCLP optimization later on
```


## 3. Load and Prepare Spatial Data (Using CSV Files)

This section loads your spatial point data (candidate locations, existing centers, demand points) from CSV files and prepares them for OSRM.

```{r load_prepare_data_csvs, message=TRUE}
message("Loading and preparing spatial data from CSVs...")

# --- IMPORTANT: Ensure your CSV files are in the 'data/' subfolder ---
# And check that they have the required columns: 'id', 'latitude', 'longitude'.
# For demand_points.csv, also ensure a 'population' or 'weight' column.

# Load Candidate Locations
candidate_locs <- read.csv(paste0(input_data_path, "candidate_locations.csv")) %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>% # crs 4326 is WGS84 (lat/lon)
  # Ensure unique 'id' column is of character type. Adjust 'id' if your CSV column is named differently.
  dplyr::mutate(id = as.character(id))

# Load Existing Cooling Center Locations
existing_locs <- read.csv(paste0(input_data_path, "existing_cooling_centers.csv")) %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  # Ensure unique 'id' column is of character type. Adjust 'id' if your CSV column is named differently.
  dplyr::mutate(id = as.character(id)) %>%
  # Add an Address column so the columns align with candidate locations dataframe as well. For now populate with NA
  dplyr::mutate(address = NA) %>%
  dplyr::select(c("id","address","type","geometry"))

# Load Demand Points
demand_pts <- read.csv(paste0(input_data_path, "demand_points.csv")) %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  # Ensure unique 'id' column is of character type. Adjust 'id' if your CSV column is named differently.
  dplyr::mutate(id = as.character(id))



# Combine all potential facility locations (candidate + existing)
# This combined set will be the 'sources' for your OSRM queries.

all_facilities <- dplyr::bind_rows(candidate_locs, existing_locs) %>%
  # Add a simple numeric_id for consistent internal matrix indexing,
  # although we will primarily use the 'id' column for clarity in results.
  dplyr::mutate(numeric_id = dplyr::row_number())

# Optional: Basic check for unique IDs (good practice)
if (length(unique(all_facilities$id)) != nrow(all_facilities)) {
  warning("Duplicate 'id' values found in combined all_facilities. This might cause issues with matrix naming.")
}
if (length(unique(demand_pts$id)) != nrow(demand_pts)) {
  warning("Duplicate 'id' values found in demand_pts. This might cause issues with matrix naming.")
}

message("Spatial data loaded and prepared. Sample of all_facilities:")
print(head(all_facilities))
message("Sample of demand_pts:")
print(head(demand_pts))
```


## 4. Define Helper Function for OSRM Queries
This helper function wraps `osrmTable` calls, adding robustness through chunking queries and retries. This is crucial for large datasets. The code in this section defines a single function that acts as a robust wrapper around the standard osrmTable() function from the osrm package. Its main purpose is to make the process of querying the OSRM servers for large datasets more reliable and less prone to crashing or timing out.

This is achieved through two key mechanisms:

Chunking: It splits your large set of destination points (dst_sf) into smaller, more manageable chunks before sending them to the OSRM server.

Error Handling and Retries: It wraps each query in a tryCatch block, so if a query fails (e.g., due to a network hiccup or a server timeout), it automatically retries it a few times before giving up.

Function Arguments

First, let's look at the arguments the function accepts. These are the inputs you provide when you call it:

src_sf: The sf object for your source locations (your combined all_facilities).

dst_sf: The sf object for your destination locations (your demand_pts).

measure: What you want to calculate, e.g., c("distance", "duration").

server_url, server_port: The address and port of your local Docker OSRM server.

chunk_size: How many destination points to send in a single query.

retries: How many times to retry a failed query.

```{r osrm_helper_function}

safe_osrmTable <- function(src_sf, dst_sf, measure = c("distance", "duration"),
                           server_url, server_port, chunk_size = 500, retries = 3) {

  message(paste0("Attempting to connect to OSRM server: ", server_url, ":", server_port))
  osrm::osrm_server(url = server_url, osrm_port = server_port)

  ## Section below is used to ensure checks the source and destination sf objects to ensure they have data and set up the 
  ## number of chunks needed to process the data
  num_destinations <- nrow(dst_sf)
  num_sources <- nrow(src_sf)

  if (num_destinations == 0 || num_sources == 0) {
    warning("Source or Destination SF object is empty. Returning empty matrices.")
    return(list(distances = matrix(NA, nrow = num_sources, ncol = num_destinations,
                                   dimnames = list(src_sf$id, dst_sf$id)),
                durations = matrix(NA, nrow = num_sources, ncol = num_destinations,
                                   dimnames = list(src_sf$id, dst_sf$id))))
  }

  all_distances_chunks <- list()
  all_durations_chunks <- list()

  num_chunks <- ceiling(num_destinations / chunk_size)

  # This is the core of the function. The for loop iterates through each chunk of your demand points. Inside the loop, a while loop combined with tryCatch attempts to make a query for that chunk. If the query fails (due to a timeout, a bad connection, etc.), tryCatch "catches" the error and the while loop restarts the process, trying again up to the number of times you specified in the retries argument. 
  
  for (i in 1:num_chunks) {
    # Determines the start and end indices for the current chunk of destination points.
    start_idx <- (i - 1) * chunk_size + 1
    end_idx <- min(i * chunk_size, num_destinations)
    current_dst_chunk <- dst_sf[start_idx:end_idx, ]

    message(paste0("  Processing destination chunk ", i, " of ", num_chunks,
                   " (destinations ", start_idx, "-", end_idx, ") for server ", server_url))
    
    # This is the retry logic.
    attempt <- 1
    success <- FALSE
    while (attempt <= retries && !success) {
      tryCatch({
        # This is the actual call to the OSRM server for a single chunk.
        od_chunk <- osrmTable(src = src_sf, dst = current_dst_chunk, measure = measure)
        all_distances_chunks[[i]] <- od_chunk$distances
        all_durations_chunks[[i]] <- od_chunk$durations
        success <- TRUE
      }, error = function(e) {
        # If the query fails, it prints a warning, increments the attempt counter,
        # waits for 5 seconds, and then the while loop tries again.
        warning(paste0("    OSRM query failed (attempt ", attempt, "): ", e$message, " Retrying..."))
        attempt <<- attempt + 1
        Sys.sleep(5) # Wait 5 seconds before retrying
        if (attempt > retries) {
          stop(paste0("OSRM query failed after multiple retries for destination chunk ", i, ". Aborting."))
        }
      })
    }
  }

  final_distances <- do.call(cbind, all_distances_chunks)
  final_durations <- do.call(cbind, all_durations_chunks)

  rownames(final_distances) <- src_sf$id
  colnames(final_distances) <- dst_sf$id[1:ncol(final_distances)]
  rownames(final_durations) <- src_sf$id
  colnames(final_durations) <- dst_sf$id[1:ncol(final_durations)]

  message(paste0("Successfully generated matrices for server: ", server_url))
  return(list(distances = final_distances, durations = final_durations))
}
```

## 5. Calculate Network Distance Matrices using OSRM


